{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower_Classifier_Colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "LIOtVKjvO8hL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports section\n",
        "import torch, torchvision, PIL, json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np, os, seaborn as sns\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import nn, optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdqF4qJdRkmX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# uninstall older PIL version\n",
        "!pip uninstall -y Pillow\n",
        "# install the new one\n",
        "!pip install Pillow\n",
        "# check Pillow version\n",
        "# restart might be needed to use the new version\n",
        "print(PIL.PILLOW_VERSION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yShq_fjDR480",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "\n",
        "# If CUDA is not available, it can be enabled from \n",
        "# Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I7JOOVcaTmoi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the Flower Dataset from https://www.kaggle.com/lenine/flower-102diffspecies-dataset\n",
        "# Also, the category to name json file\n",
        "!wget -cq https://github.com/udacity/pytorch_challenge/raw/master/cat_to_name.json\n",
        "!wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
        "!rm -r flower_data || true\n",
        "!unzip -qq flower_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1l6rGTh0TqBD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the training and validation directories\n",
        "data_dir = 'flower_data'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/valid'\n",
        "\n",
        "# Paramns\n",
        "scale = 256\n",
        "input_shape = 224\n",
        "mean = [0.485, 0.456, 0.406] # ImageNet parameters\n",
        "std = [0.229, 0.224, 0.225] # ImageNet parameters\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFxGfSv040Wt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the transforms for the training and validation sets\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(20),\n",
        "                                       transforms.RandomResizedCrop(input_shape),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean, std)])\n",
        "valid_transforms = transforms.Compose([transforms.Resize(scale),\n",
        "                                      transforms.CenterCrop(input_shape),\n",
        "                                      transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean, std)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_NdtXc-IwHw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the datasets with ImageFolder\n",
        "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "valid_data = datasets.ImageFolder(valid_dir, transform=valid_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n1uNl0nBI0wP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the dataloaders using the image datasets and the trainforms\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "validloader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sd4jVlzSS2E8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a method to check the images from the dataloaders\n",
        "def imgloader(img):\n",
        "  plt.figure(figsize=(25,15))\n",
        "  img=img.numpy()\n",
        "  img[0,:,:]=img[0,:,:]*0.229+0.485\n",
        "  img[1,:,:]=img[1,:,:]*0.224+0.456\n",
        "  img[2,:,:]=img[2,:,:]*0.225+0.406\n",
        "  plt.imshow(np.transpose(img,(1,2,0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PR9xZS24TPvd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the trainloader data\n",
        "# TODO: Reposition the grid so that it splits the images\n",
        "images,labels=next(iter(trainloader))\n",
        "img=torchvision.utils.make_grid(images[:32])\n",
        "imgloader(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "39P6oKc-TX8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the validloader data\n",
        "# TODO: Reposition the grid so that it splits the images\n",
        "images,labels=next(iter(validloader))\n",
        "img=torchvision.utils.make_grid(images[:32])\n",
        "imgloader(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HpSa-7KjRqpJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import category to name JSON data\n",
        "with open('cat_to_name.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_eRbjYlDTd0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load a pretrained model of your choice from torchvision.models\n",
        "model = models.resnet152(pretrained=True)\n",
        "\n",
        "# Check model architecture so you can define your new classifier\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lv80gw22Wtfv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a new untrained feed-forward classifier, using ReLu activations and droptout\n",
        "# Keep input features at 2048, change output features from 1000 to 102 (# of flower categories)\n",
        "\n",
        "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(0.2),\n",
        "                            nn.Linear(512, 102),\n",
        "                            nn.LogSoftmax(dim=1))\n",
        "\n",
        "# Map classes to indices to be used for Inference\n",
        "model.class_to_idx = train_data.class_to_idx\n",
        "\n",
        "# Check model with the new fc classifier\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54wxR4Xa6N01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Image Preprocessing\n",
        "# Scales, crops, and normalizes a PIL image for a PyTorch model, returns an Numpy array\n",
        "\n",
        "def process_image(image_path):\n",
        "\n",
        "  # Open the image\n",
        "  from PIL import Image\n",
        "  img = Image.open(image_path)\n",
        "  \n",
        "  # Resize\n",
        "  if img.size[0] > img.size[1]:\n",
        "    img.thumbnail((10000, 256))\n",
        "  else:\n",
        "    img.thumbnail((256, 10000))\n",
        "    \n",
        "  # Crop \n",
        "  left_margin = (img.width-224)/2\n",
        "  bottom_margin = (img.height-224)/2\n",
        "  right_margin = left_margin + 224\n",
        "  top_margin = bottom_margin + 224\n",
        "  img = img.crop((left_margin, bottom_margin, right_margin,   \n",
        "                      top_margin))\n",
        "  # Normalize\n",
        "  img = np.array(img)/256\n",
        "  img = (img - mean)/std\n",
        "    \n",
        "  # Move color channels to first dimension as expected by PyTorch\n",
        "  img = img.transpose((2, 0, 1))\n",
        "    \n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0dLKLvh6b8w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert Pytorch tensor and display it\n",
        "\n",
        "def imshow(image, ax=None, title=None):\n",
        "\n",
        "  # aesthetics configuration\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots();\n",
        "  if title:\n",
        "    plt.title(title);\n",
        "    \n",
        "  # PyTorch tensors assume the color channel is first\n",
        "  # but matplotlib assumes is the third dimension\n",
        "  image = image.transpose((1, 2, 0))\n",
        "    \n",
        "  # Undo preprocessing\n",
        "  image = std * image + mean\n",
        "    \n",
        "  # Image needs to be clipped between 0 and 1\n",
        "  image = np.clip(image, 0, 1)\n",
        "    \n",
        "  ax.imshow(image);\n",
        "    \n",
        "  return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F2WOZye46e-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predict top(5) class probabilities using the model\n",
        "\n",
        "def predict(image_path, model, top_num=5):\n",
        "\n",
        "    # Process image\n",
        "    img = process_image(image_path)\n",
        "    \n",
        "    # Model activation for the eval mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Numpy -> Tensor\n",
        "    image_tensor = torch.from_numpy(img).type(torch.FloatTensor)\n",
        "    \n",
        "    # Add batch of size 1 to image\n",
        "    model_input = image_tensor.unsqueeze(0)\n",
        "    \n",
        "    # CPU transfer\n",
        "    image_tensor.to('cpu')\n",
        "    model_input.to('cpu')\n",
        "    model.to('cpu')\n",
        "    \n",
        "    # Probs\n",
        "    probs = torch.exp(model.forward(model_input))\n",
        "    \n",
        "    # Top probs\n",
        "    top_probs, top_labs = probs.topk(top_num)\n",
        "    top_probs = top_probs.detach().numpy().tolist()[0] \n",
        "    top_labs = top_labs.detach().numpy().tolist()[0]\n",
        "    \n",
        "    # Convert indices to classes\n",
        "    idx_to_class = {val: key for key, val in    \n",
        "                                      model.class_to_idx.items()}\n",
        "    top_labels = [idx_to_class[lab] for lab in top_labs]\n",
        "    top_flowers = [cat_to_name[idx_to_class[lab]] for lab in top_labs]\n",
        "    return top_probs, top_labels, top_flowers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c0EIVabB6gqB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot the image with the title and probabilities for the top 5 classes as a bar graph\n",
        "\n",
        "def plot_solution(image_path, model):\n",
        "    \n",
        "    # Set up plot\n",
        "    plt.figure(figsize = (6,10))\n",
        "    ax = plt.subplot(2,1,1)\n",
        "    \n",
        "    # Set up title\n",
        "    flower_num = image_path.split('/')[2] \n",
        "    title_ = cat_to_name[str(flower_num)] \n",
        "    \n",
        "    # Plot flower\n",
        "    img = process_image(image_path)\n",
        "    imshow(img, ax, title = title_);\n",
        "    \n",
        "    # Make prediction\n",
        "    probs, labs, flowers = predict(image_path, model) \n",
        "    \n",
        "    # Plot bar chart\n",
        "    plt.subplot(2,1,2);\n",
        "    sns.barplot(x=probs, y=flowers, color=sns.color_palette()[0]);\n",
        "    plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wuCMCzNX6jpW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test the model before training the Classifier\n",
        "# Top 5 probabilities should be close to the same size at ~1%\n",
        "\n",
        "image_path = os.path.join(valid_dir, '102/image_08040.jpg')\n",
        "plot_solution(image_path, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PXga3kacNUVh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Freeze all model parameters except the untrained newly created classifier\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "   \n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CGnZz0ZuNvVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the criterion and optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-2xfbIZ7O1Lv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the device on which the training will be performed: cuda or cpu\n",
        "# If cuda is available it's best to use it to improve trainig speed by over 100x\n",
        "device=torch.device((\"cuda\") if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ai_3LdRVQ2yE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the function for training the model and saving the checkpoint when\n",
        "# Test/Validation loss decreases\n",
        "def train_model():\n",
        "  \n",
        "    epochs = 100\n",
        "    steps = 0\n",
        "    training_loss = 0\n",
        "    print_every = 20\n",
        "    valid_loss_min = np.Inf\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, labels in trainloader:\n",
        "            steps += 1\n",
        "            # Move input and label tensors to the device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)    \n",
        "            optimizer.zero_grad()\n",
        "            logps = model(inputs)\n",
        "            loss = criterion(logps, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.item()\n",
        "            # Validate model and print results after print_every number of steps\n",
        "            if steps % print_every == 0:\n",
        "                valid_loss = 0\n",
        "                accuracy = 0\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in validloader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        logps = model(inputs)\n",
        "                        batch_loss = criterion(logps, labels)\n",
        "                        valid_loss += batch_loss.item()\n",
        "\n",
        "                        # Calculate accuracy\n",
        "                        ps = torch.exp(logps)\n",
        "                        top_p, top_class = ps.topk(1, dim=1)\n",
        "                        equals = top_class == labels.view(*top_class.shape)\n",
        "                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "                valid_loss = valid_loss/len(validloader)\n",
        "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "                      f\"Train loss: {training_loss/print_every:.3f}.. \"\n",
        "                      f\"Test loss: {valid_loss:.3f}.. \"\n",
        "                      f\"Test accuracy: {accuracy/len(validloader):.3f}\")\n",
        "                # Save the model if validation loss has decreased\n",
        "                if valid_loss <= valid_loss_min:\n",
        "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "                    valid_loss_min,\n",
        "                    valid_loss))\n",
        "                    model.class_to_idx = train_data.class_to_idx\n",
        "                    torch.save({\n",
        "                      'arch':'resnet152',\n",
        "                      'model_state_dict': model.state_dict(),\n",
        "                      'optimizer_state_dict': optimizer.state_dict(),\n",
        "                      'validation_accuraccy': {accuracy/len(validloader)},\n",
        "                      'class_to_idx': model.class_to_idx,\n",
        "                      'validation_loss': {valid_loss},\n",
        "                      'train_loss': {training_loss/print_every},\n",
        "                      }, 'checkpoint.pth')\n",
        "                    valid_loss_min = valid_loss\n",
        "                training_loss = 0\n",
        "                model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfStyT-TfHb4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train the model's classifier\n",
        "# After 1 epoch the Accuracy is close to 70% and after 3 epochs close to 90%.\n",
        "# As the increase in accuracy/decrease in test loss happen at a much lower rate,\n",
        "# training should be stopped and new approaches should be used.\n",
        "train_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F9SAZ4oCYkvZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a method for loading the checkpoint and rebuilding the model\n",
        "def load_checkpoint(checkpoint_file):\n",
        "  checkpoint = torch.load(checkpoint_file)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "  model.class_to_idx = checkpoint['class_to_idx']\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Nq5VNpFZkRW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the saved checkpoint for further training or inference\n",
        "load_checkpoint('checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAen944qYc5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test the model after training the Classifier\n",
        "# Model should be able to predict the correct flower with high accuracy\n",
        "\n",
        "image_path = os.path.join(valid_dir, '102/image_08040.jpg')\n",
        "plot_solution(image_path, model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}